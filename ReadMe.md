# DeepFake Detection Models

This repository contains code on detecting DeepFakes using a hybrid CNNâ€“LSTM architecture. The goal is to distinguish real from fake videos by extracting spatial features with ResNeXtâ€‘50 and modeling temporal dynamics with LSTM layers.

I train and evaluate the same model on two datasets:

1. **DFDC (DeepFake Detection Challenge)**
2. **AvLips (LipSync) / LipSync Dataset**

---

## ğŸ“‹ Table of Contents

* [ğŸš€ Overview](#-overview)
* [ğŸ“‚ Repository Structure](#-repository-structure)
* [âš™ï¸ Environment Setup](#ï¸-environment-setup)
* [ğŸ“¥ Data Preparation](#-data-preparation)

  * [DFDC Dataset](#dfdc-dataset)
  * [AvLips Dataset](#avlips-dataset)
* [ğŸƒâ€â™‚ï¸ Training Instructions](#-training-instructions)

  * [Train on DFDC](#train-on-dfdc)
  * [Train on AvLips](#train-on-avlips)
* [ğŸ“Š Evaluation & Metrics](#-evaluation--metrics)
* [ğŸ“ˆ Results Visualization](#-results-visualization)
* [ğŸ› ï¸ Dependencies](#ï¸-dependencies)
* [ğŸ¤ Contributing](#-contributing)
* [ğŸ“ License](#-license)

---

## ğŸš€ Overview

As part of my undergraduate capstone, I built a DeepFake detection pipeline. It:

* Samples and preprocesses video frames uniformly
* Extracts frame-level features using a pre-trained ResNeXtâ€‘50 backbone
* Refines features with residual blocks
* Models frame sequences with an LSTM
* Outputs binary predictions (real vs. fake)
* Reports accuracy, precision, recall, F1-score, and confusion matrices

---

## ğŸ“‚ Repository Structure

```bash
â”œâ”€â”€ data/                 
â”‚   â”œâ”€â”€ dfdc_dataset/          
â”‚   â””â”€â”€ avlips_dataset/       
â”œâ”€â”€ deepfake_detection_dfdc.ipynb   
â”œâ”€â”€ deepfake_detection_AvLips.ipynb   
â”œâ”€â”€ requirements.txt           
â””â”€â”€ README.md                
```

---

## âš™ï¸ Environment Setup

1. Clone this repo:

   ```bash
   git clone https://github.com/sangeetanandanvishal04/deepfake_detection.git
   cd deepfake_detection
   ```
2. Install Python packages:

   ```bash
   pip install -r requirements.txt
   ```

> **Note**: For GPU training on Colab, use the CUDA build:
>
> ```bash
> pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118
> ```

---

## ğŸ“¥ Data Preparation

### DFDC Dataset

* The notebook downloads `dfdc_dataset.zip` via `gdown` and extracts into `data/dfdc_dataset/real` and `.../fake`.

### AvLips Dataset

* Provide your AvLips archiveâ€™s Google Drive file ID in the notebook.
* The same download-and-extract flow populates `data/avlips_dataset/real` and `.../fake`.

---

## ğŸƒâ€â™‚ï¸ Training Instructions

Adjust hyperparameters at the top of the notebook:

```python
BATCH_SIZE = 8
NUM_EPOCHS = 10
learning_rate = 1e-5
weight_decay = 1e-6
```

### Train on DFDC

1. Ensure `DATASET_FOLDER = Path("data/dfdc_dataset")`.
2. Run all cells to preprocess, define model, and train.

### Train on AvLips

1. Change to `DATASET_FOLDER = Path("data/avlips_dataset")`.
2. Run preprocessing and training cells again.

---

## ğŸ“Š Evaluation & Metrics

After training, the notebook computes these metrics on both train/test splits:

* Loss (training vs. test)
* Accuracy
* Precision
* Recall
* F1â€‘score
* Confusion matrix

---

## ğŸ“ˆ Results Visualization

Loss and accuracy curves are plotted using `matplotlib`. You can extend this to TensorBoard or other visualization tools.

---

## ğŸ› ï¸ Dependencies

See `requirements.txt`:

```
torch
torchvision
torchaudio
gdown
matplotlib
scikit-learn
opencv-python
torchinfo
tqdm
Pillow
```

---

## ğŸ¤ Contributing

As a fellow student or researcher, your feedback is welcome! Feel free to open issues or pull requests.

---

## ğŸ“ License

This project is released under the MIT License. See [LICENSE](LICENSE) for details.